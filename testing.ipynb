# Import necessary modules
import os
import pathlib
import pandas as pd
import tools
from rich import print
from collections import Counter
from string import punctuation
from sklearn.feature_extraction.text import TfidfVectorizer

# Get the list of texts to read
texts_to_read = sorted(os.listdir("../data"))

# Initialize the document source and corpus-related lists
doc = tools.Source(pathlib.PosixPath("../data/"), pathlib.PosixPath("../"))
corpus_names = []
corpii = []
rmpunc = []
tok = []
swdrop = []
nouns = []
lemma = []
vector = []

# Read and preprocess each text in the list
for text in texts_to_read:
    # Name of the source
    key = text.split(".")[0]
    corpus_names.append(key)

    # Read the corpus content from the file
    with open(os.path.join(doc.source, text), "r") as f:
        corpus_stack = f.read()

        # Preprocess the corpus content
        corpus_stack = corpus_stack\
            .lower()\
            .replace("-\n", "")\
            .replace("•", "")\
            .replace("’", "")\
            .replace("“", "")\
            .replace("”", "")\
            .replace("‘", "")\
            .replace("\n\n", " ")\
            .replace("\n", " ")\
            .replace("  ", " ")

        # Append the preprocessed corpus content to the respective lists
        corpii.append(corpus_stack)
        rmpunc.append("".join([x for x in str(corpus_stack) if x not in punctuation]).replace("  ", " "))
        tok.append(doc.tokenise(rmpunc[-1]))
        swdrop.append(doc.drop_stopwords(tok[-1], alpha_only=True, len_limit=1))
        nouns.append(doc.extract_nouns(rmpunc[-1]))
        lemma.append(doc.lemmatise(nouns[-1]))
        vector.append(" ".join(lemma[-1]))

# Create a dataframe with the corpus content
df = pd.DataFrame({
    "source": corpus_names,
    "text": corpii,
    "punct_removed": rmpunc,
    "tokenised": tok,
    "stopwords_dropped": swdrop,
    "nouns": nouns,
    "lemmatised": lemma,
    "vectorised": vector
})

# Display the first 10 rows of the dataframe
df.head(10)
